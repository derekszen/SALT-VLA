# SALT-VLA Progress Snapshot
# Last updated: 2026-02-01

Goal
- Train a hybrid ST-Transformer student to predict frozen VideoMAE-H teacher latents (cached) on SSv2, then evaluate with lightweight probes.

Current implementation status
- Data: SSv2 loader with deterministic sampling + transforms, returns [B, 3, 16, 224, 224].
- Teacher: HF VideoMAE-H wrapper, CLS dropped, fixed projection W 1280->384 saved to disk for caching.
- Cache: zarr targets [num_samples, 1568, 384] float16 + meta.jsonl mapping.
- Student: D=384, depth=12, heads=6; hybrid attention (8 factorized + 4 joint) + 4-layer predictor.
- Training: tube masking (shared spatial mask across time), cosine loss on masked tokens, AMP + DDP hooks, cached targets only.
- Eval: UCF101 linear probe + MSR-VTT/MSVD retrieval scripts.
- Scripts: build cache, pretrain, eval UCF101, eval retrieval, UCF101 quick inference.

Milestones
- M0: Environment imports validated (torch/transformers/einops/decord).
- M1: SSv2 loader implemented + unit test.
- M2: Teacher wrapper + projection implemented + unit test.
- M3: Cache builder + roundtrip test implemented.
- M4: Student hybrid ST-Transformer implemented + unit test.
- M5: Pretraining loop implemented; overfit-32 mode available (not yet run on real data).
- M6: DDP support implemented (not yet validated on 8 GPUs).
- M7: Downstream eval scripts implemented (not yet validated end-to-end).

Default paths
- SSv2: /mnt/ssv2
- Cache: /mnt/ssv2/cache_videomae_huge_384
- UCF101: /mnt/ucf101

Next concrete actions
- Build a 1k-sample cache shard and run the overfit-32 check to validate M5.
- Validate DDP via torchrun on multi-GPU if available.
- Run UCF101 linear probe and retrieval scripts on small subsets for sanity.
